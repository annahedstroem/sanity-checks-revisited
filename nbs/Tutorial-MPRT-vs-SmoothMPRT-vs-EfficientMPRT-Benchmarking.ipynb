{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d4132a",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "\n",
    "In this notebook, we provide the code for running and collecting experimental results as described in the paper:\n",
    "\n",
    "[**Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation Test**](https://openreview.net/forum?id=vVpefYmnsG) by Anna Hedström, Leander Weber, Sebastian Lapuschkin, Marina Höhne. \n",
    "\n",
    "Please note that this notebook assumes access to test sets for the `MNIST`, `fMNIST`, and `ImageNet` datasets. If you haven't generated the necessary data yet, please refer to our tutorial [here](https://github.com/annahedstroem/MetaQuantus/blob/main/tutorials/Tutorial-Data-Generation-Experiments.ipynb) to generate the required dataset.\n",
    "\n",
    "Make sure to update file paths as needed before running the experiments (e.g., `nbs/temp_results/` and `nbs/temp_results/plots/`. Also enable GPU as some experiments are quite computationally costly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f24840",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc6c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install -r ../requirements.txt\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8548fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load supporting functions.\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src import *\n",
    "\n",
    "# Import libraries.\n",
    "import quantus\n",
    "import metaquantus\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    print('Using device:', torch.cuda.get_device_name(0))\n",
    "    !nvidia-smi\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02632a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths to datasets and to write results.\n",
    "path_assets =  \"../../MetaQuantus/assets/\"\n",
    "path_results = \"temp_results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f74e8a",
   "metadata": {},
   "source": [
    "## MPRT vs eMPRT analysis\n",
    "\n",
    "This cell loads datasets, models, explanations and parameterise the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets, models and explanations.\n",
    "datasets = {\n",
    "    \"MNIST_LeNet\": {\n",
    "        \"batch_size\": 1000,\n",
    "        \"full_size\": 1000,\n",
    "        \"fig_size\": (4, 4),\n",
    "    },\n",
    "    \"fMNIST_LeNet\": {\n",
    "        \"batch_size\": 1000,\n",
    "        \"full_size\": 1000,\n",
    "        \"fig_size\": (4, 4),\n",
    "    },\n",
    "    \"ImageNet_ResNet18\": {\n",
    "        \"batch_size\": 100,\n",
    "        \"full_size\": 300,\n",
    "        \"fig_size\": (8, 4),\n",
    "    },\n",
    "    \"ImageNet_VGG16\": {\n",
    "        \"batch_size\": 25,\n",
    "        \"full_size\": 300,\n",
    "        \"fig_size\": (8, 4),\n",
    "    },\n",
    "}\n",
    "xai_methods = [\n",
    "    \"Control Var. Random Uniform\",\n",
    "    \"Gradient\",\n",
    "    \"Saliency\",\n",
    "    \"LayerGradCam\",\n",
    "    \"SmoothGrad\",\n",
    "    \"IntegratedGradients\",\n",
    "    \"LRP-Eps\",\n",
    "    \"LRP-Z+\",\n",
    "    \"Guided-Backprop\",\n",
    "    \"GradientShap\",\n",
    "    \"InputXGradient\",\n",
    "]\n",
    "\n",
    "# Parameterise the metrics.\n",
    "metrics = [\n",
    "    quantus.EfficientMPRT(\n",
    "        layer_order=\"bottom_up\",\n",
    "        complexity_func=quantus.complexity_func.discrete_entropy,\n",
    "        complexity_func_kwargs={\"n_bins\": 100},\n",
    "        similarity_func=quantus.correlation_spearman,\n",
    "        skip_layers=False,\n",
    "        abs=False,\n",
    "        normalise=True,\n",
    "        normalise_func=quantus.normalise_by_average_second_moment_estimate,\n",
    "        disable_warnings=True,\n",
    "        compute_extra_scores=True,\n",
    "    ),\n",
    "    quantus.SmoothMPRT(\n",
    "        layer_order=\"bottom_up\",\n",
    "        nr_samples=50,\n",
    "        noise_magnitude=0.1,\n",
    "        similarity_func=quantus.correlation_spearman,\n",
    "        skip_layers=False,\n",
    "        abs=True,\n",
    "        normalise=True,\n",
    "        normalise_func=quantus.normalise_by_average_second_moment_estimate,\n",
    "        disable_warnings=True,\n",
    "    ),\n",
    "    quantus.MPRT(\n",
    "        layer_order=\"top_down\",\n",
    "        similarity_func=quantus.correlation_spearman,\n",
    "        skip_layers=False,\n",
    "        abs=True,\n",
    "        normalise=True,\n",
    "        normalise_func=quantus.normalise_by_average_second_moment_estimate,\n",
    "        disable_warnings=True,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d2bfc",
   "metadata": {},
   "source": [
    "The cell below details a process where, if `generate_data` is set to `True`, it iterates through different metrics (`EfficientMPRT` and `MPRT`) to evaluate different explanation methods, including an extensive setup for datasets, models, and batch processing. It concludes by saving the results for each metric in a pickle file to `path_results` (i.e., `nbs/temp_results`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "generate_data = False\n",
    "\n",
    "if generate_data:\n",
    "    for metric in metrics:\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        scores = {}\n",
    "        for meta_name, meta_data in datasets.items():\n",
    "            scores[meta_name] = {}\n",
    "            dataset_name, model_name = meta_name.split(\"_\")\n",
    "\n",
    "            # Get datasst settings.\n",
    "            dataset_settings = setup_experiments(\n",
    "                dataset_name=dataset_name, path_assets=path_assets, device=device\n",
    "            )\n",
    "            estimator_kwargs = dataset_settings[dataset_name][\"estimator_kwargs\"]\n",
    "            dataset_kwargs = dataset_settings[dataset_name]\n",
    "\n",
    "            # Get the model.\n",
    "            model = dataset_kwargs[\"models\"][model_name]\n",
    "\n",
    "            # Setup for batch processing.\n",
    "            nr_batches = int(meta_data[\"full_size\"] / meta_data[\"batch_size\"])\n",
    "\n",
    "            temp_batches = {}\n",
    "            for batch_id in range(nr_batches):\n",
    "\n",
    "                # Get the data for the batch.\n",
    "                start_idx = batch_id * meta_data[\"batch_size\"]\n",
    "                end_idx = start_idx + meta_data[\"batch_size\"]\n",
    "                x_batch = dataset_kwargs[\"x_batch\"][start_idx:end_idx]\n",
    "                y_batch = dataset_kwargs[\"y_batch\"][start_idx:end_idx]\n",
    "                s_batch = dataset_kwargs[\"s_batch\"][start_idx:end_idx]\n",
    "\n",
    "                # Update model-specific xai parameters.\n",
    "                xai_methods_with_kwargs = {**setup_xai_methods_captum(\n",
    "                    xai_methods=xai_methods,\n",
    "                    x_batch=x_batch,\n",
    "                    gc_layer=dataset_kwargs[\"gc_layers\"][model_name],\n",
    "                    img_size=estimator_kwargs[\"img_size\"],\n",
    "                    nr_channels=estimator_kwargs[\"nr_channels\"],\n",
    "                    nr_segments=50,\n",
    "                ), **setup_xai_methods_zennit(xai_methods=xai_methods, model=model)}\n",
    "\n",
    "                for xai_method, xai_method_kwargs in xai_methods_with_kwargs.items():\n",
    "\n",
    "                    scores[meta_name][xai_method] = {}\n",
    "\n",
    "                    if batch_id == 0:\n",
    "                        temp_batches[xai_method] = {\n",
    "                            \"model_scores\": [], \n",
    "                            \"explanation_scores\": [], \n",
    "                            \"correlation_scores\": [],\n",
    "                            \"evaluation_scores\": [],\n",
    "                            \"similarity_scores\": [],\n",
    "                        }\n",
    "\n",
    "                    if debug:\n",
    "                        start = time.time()\n",
    "                        print(f\"batch id {batch_id} - {xai_method} ({model_name}, {dataset_name})\")\n",
    "\n",
    "                    metric.abs = True\n",
    "                    if xai_method == \"SmoothGrad\":\n",
    "                        metric.abs = True\n",
    "\n",
    "                    # Score!\n",
    "                    evaluation_scores = metric(model=model,\n",
    "                        x_batch=x_batch,\n",
    "                        y_batch=y_batch,\n",
    "                        softmax=True,\n",
    "                        a_batch=None,\n",
    "                        device=device,\n",
    "                        explain_func=quantus.explain,\n",
    "                        explain_func_kwargs={**{\"method\": xai_method}, **xai_method_kwargs},\n",
    "                    )\n",
    "\n",
    "                    # Save intermediate batch results.\n",
    "                    try:\n",
    "                        temp_batches[xai_method][\"model_scores\"].append(metric.model_scores_by_layer)\n",
    "                        temp_batches[xai_method][\"explanation_scores\"].append(metric.explanation_scores_by_layer)\n",
    "                        temp_batches[xai_method][\"correlation_scores\"].extend(metric.scores_extra[\"scores_correlation_model_vs_explanation_complexity\"])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error message: {e}\")\n",
    "\n",
    "                    if metric.name != \"Model Parameter Randomisation Test\":\n",
    "                        temp_batches[xai_method][\"similarity_scores\"].extend(evaluation_scores)\n",
    "                    else:\n",
    "                        temp_batches[xai_method][\"evaluation_scores\"].extend(evaluation_scores)\n",
    "\n",
    "                    if debug:\n",
    "                        for k, v in temp_batches[xai_method].items():\n",
    "                            try:\n",
    "                                if all((isinstance(val, (int, float)) and (val == 0 or np.isnan(val))) for val in v):\n",
    "                                    print(f\"\\t\\tWarning: '{k}' - with all NaNs or zeros!\")\n",
    "                            except TypeError:\n",
    "                                pass \n",
    "                            if isinstance(v, list) and not v:\n",
    "                                print(f\"\\t\\tWarning:{k} - list is empty!\")\n",
    "\n",
    "                        print(f\"\\t\\t\\t{evaluation_scores}\")\n",
    "                        print(f\"\\tTime to compute {time.time()-start:.2f} secs.\")\n",
    "\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            # Merge dictionaries for different batches.\n",
    "            for xai_method, xai_method_kwargs in xai_methods_with_kwargs.items():\n",
    "                for k in temp_batches[xai_method].keys():\n",
    "                    if isinstance(temp_batches[xai_method][k][0], dict):\n",
    "\n",
    "                        # Merge all dicts according the the same key!\n",
    "                        merged_dict = defaultdict(list)\n",
    "                        for d in temp_batches[xai_method][k]:\n",
    "                            for inner_k, v in d.items():\n",
    "\n",
    "                                # Extend the list of float values.\n",
    "                                merged_dict[inner_k].extend(v)  \n",
    "\n",
    "                        # Replace the list of dictionaries with the merged dictionary.\n",
    "                        temp_batches[xai_method][k] = dict(merged_dict)\n",
    "\n",
    "            # Put the results back to scores dictionary.\n",
    "            for xai_method, xai_method_kwargs in xai_methods_with_kwargs.items():\n",
    "                for k in temp_batches[xai_method].keys():\n",
    "                    scores[meta_name][xai_method] = temp_batches[xai_method]\n",
    "\n",
    "        # Save data for each metric.\n",
    "        with open(path_results+f\"{metric.name}_curves_scores.pickle\", 'wb') as f:\n",
    "            pickle.dump(scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a916d5",
   "metadata": {},
   "source": [
    "### Plot MPRT and eMPRT curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6a636",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_curves(path_results: str,\n",
    "                metric_name: str = \"MPRT\",\n",
    "                plotting_func: Callable = plot_randomisation_curves,\n",
    "                ):\n",
    "    # Load data.\n",
    "    with open(path_results+f\"{metric_name}_curves_scores.pickle\", 'rb') as f:\n",
    "        scores = pickle.load(f)\n",
    "\n",
    "    for meta_name, meta_data in datasets.items():\n",
    "        dataset_name, model_name = meta_name.split(\"_\")\n",
    "\n",
    "        # Plot!\n",
    "        \n",
    "        plot_randomisation_curves(scores[meta_name],\n",
    "                    dataset_name,\n",
    "                    model_name,\n",
    "                    metric_name=metric_name,\n",
    "                    figsize=meta_data[\"fig_size\"]) # (10, 6)) (min(len(list(model.modules())), 10), 4)\n",
    "\n",
    "        plt.savefig(path_results+f\"plots/{metric_name}_curves_{dataset_name}_{model_name}.svg\")\n",
    "        plt.savefig(path_results+f\"plots/{metric_name}_curves_{dataset_name}_{model_name}.pdf\")\n",
    "        plt.show()\n",
    "\n",
    "plot_curves(path_results, metric_name=\"eMPRT\")\n",
    "plot_curves(path_results, metric_name=\"MPRT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb2f7e4",
   "metadata": {},
   "source": [
    "### Plot eMPRT aggregation over tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load eMPRT data.\n",
    "with open(path_results+f\"eMPRT_curves_scores.pickle\", 'rb') as f:\n",
    "    scores_emprt = pickle.load(f)\n",
    "    \n",
    "# Restructure data.\n",
    "scores_df = {}\n",
    "for meta_name, meta_data in datasets.items():\n",
    "    scores_df[meta_name] = {}\n",
    "    for xai_method in xai_methods:\n",
    "        emprt_scores = scores_emprt[meta_name][xai_method][\"rate_of_change_scores\"]\n",
    "        scores_df[meta_name][xai_method] = f\"{np.mean(emprt_scores):.3f} $\\pm$ {np.std(emprt_scores):.3f}\"\n",
    "\n",
    "# Get the rankings.\n",
    "df_ranking = pd.DataFrame(scores_df)\n",
    "df_ranking = df_ranking[['ImageNet_VGG16', 'ImageNet_ResNet18', 'MNIST_LeNet', 'fMNIST_LeNet']]\n",
    "means = df_ranking.applymap(lambda x: float(x.split(' $\\\\pm$ ')[0]))\n",
    "stds = df_ranking.applymap(lambda x: float(x.split(' $\\\\pm$ ')[1]))\n",
    "\n",
    "# Configs.\n",
    "num_methods = len(df_ranking.index)\n",
    "bar_width = 0.085\n",
    "r = np.arange(len(df_ranking.columns))\n",
    "bottom = 0.5\n",
    "\n",
    "# Plot bar!\n",
    "fig, axs = plt.subplots(1, 1, figsize=(7, 4))\n",
    "\n",
    "for idx, method in enumerate(df_ranking.index):\n",
    "    plt.bar(r + idx * bar_width, means.loc[method]+bottom, width=bar_width,\n",
    "            color=COLOR_MAP[method], edgecolor=(0, 0, 0, 0.9), label=LABEL_MAP[method],  yerr=stds.loc[method])#, alpha=0.9) #hatch=hatches\n",
    "\n",
    "y_labels = [0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "plt.yticks(ticks=[l+bottom for l in y_labels], labels=np.array(y_labels).astype(str))\n",
    "x_labels = [f'{c.split(\"_\")[0]}\\n{c.split(\"_\")[1]}' for c in df_ranking.columns]\n",
    "plt.xticks([r + bar_width * (num_methods / 2) for r in range(len(df_ranking.columns))], x_labels)\n",
    "#plt.ylim(0, 3.7)\n",
    "plt.ylabel('eMPRT')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.grid()\n",
    "plt.savefig(path_results+f\"plots/eMPRT_benchmarking.svg\")\n",
    "plt.savefig(path_results+f\"plots/eMPRT_benchmarking.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b0c83",
   "metadata": {},
   "source": [
    "### Plot MPRT vs eMPRT difference in relative ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(scores: dict,\n",
    "           datasets: dict,\n",
    "           xai_methods_with_kwargs: dict,\n",
    "           key: str) -> pd.DataFrame:\n",
    "    scores_df = {}\n",
    "    for meta_name, meta_data in datasets.items():\n",
    "        scores_df[meta_name] = {}\n",
    "        for xai_method in xai_methods:\n",
    "            emprt_scores = scores[meta_name][xai_method][key]\n",
    "            scores_df[meta_name][xai_method] = f\"{np.mean(emprt_scores):.10f} $\\pm$ {np.std(emprt_scores):.10f}\"\n",
    "\n",
    "    return pd.DataFrame(scores_df)\n",
    "\n",
    "# Load MPRT data.\n",
    "with open(path_results+f\"MPRT_curves_scores.pickle\", 'rb') as f:\n",
    "    scores_mprt = pickle.load(f)\n",
    "    \n",
    "# Load the MPRT data.\n",
    "df_mprt = get_df(scores=scores_mprt, datasets=datasets, xai_methods_with_kwargs=xai_methods, key=\"evaluation_scores\")\n",
    "df_mprt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71544d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the eMPRT data.\n",
    "df_emprt = get_df(scores=scores_emprt, datasets=datasets, xai_methods_with_kwargs=xai_methods, key=\"rate_of_change_scores\")\n",
    "df_emprt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract means.\n",
    "means_emprt = df_emprt.applymap(lambda x: float(x.split(' $\\\\pm$ ')[0]))\n",
    "means_mprt = df_mprt.applymap(lambda x: float(x.split(' $\\\\pm$ ')[0]))\n",
    "\n",
    "# Rank the methods based on the scores.\n",
    "mprt_ranks = means_mprt.rank(axis=0, ascending=True)\n",
    "emprt_ranks = means_emprt.rank(axis=0, ascending=False)\n",
    "\n",
    "# Plot ranking!\n",
    "fig, axs = plt.subplots(1, len(datasets), figsize=(6, 4))\n",
    "\n",
    "labels = [f'{c.split(\"_\")[0]}\\n{c.split(\"_\")[1]}' for c in df_emprt.columns]\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    for xai_method in mprt_ranks.index:\n",
    "        axs[idx].plot(['MPRT', 'eMPRT'],\n",
    "                     [mprt_ranks.loc[xai_method, dataset], emprt_ranks.loc[xai_method, dataset]],\n",
    "                      label=LABEL_MAP[xai_method], color=COLOR_MAP[xai_method],\n",
    "                      linestyle='-', linewidth=8, alpha=0.9)\n",
    "\n",
    "    axs[idx].set_title(f\"{labels[idx]}\")\n",
    "    axs[idx].invert_yaxis()\n",
    "    axs[idx].grid(axis='y')\n",
    "    axs[idx].set_xticklabels(labels=['MPRT', 'eMPRT'], rotation=45)\n",
    "\n",
    "    if idx > 0:\n",
    "        axs[idx].set_yticklabels([])\n",
    "        axs[idx].set_ylabel('')\n",
    "    else:\n",
    "        axs[idx].set_yticks(labels=[\"R\"+str(int(i)) for i in np.arange(1, 12)], ticks=[i for i in np.arange(1, 12)])\n",
    "\n",
    "axs[-1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "axs[0].set_ylabel(\"Categorical Ranking (desc)\")\n",
    "plt.savefig(path_results+f\"plots/MPRT_vs_eMPRT_ranking.svg\", bbox_inches='tight')\n",
    "plt.savefig(path_results+f\"plots/MPRT_vs_eMPRT_ranking.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba6d60e",
   "metadata": {},
   "source": [
    "### Benchmarking\n",
    "\n",
    "Make sure to enable GPU for benchmarking, e.g., via colab or cluster environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f020109bb585d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental setup.\n",
    "xai_methods = {\n",
    "    \"G_GC\": [\"Gradient\", \"LayerGradCam\"],\n",
    "    \"SA_LRPplus_IXG\": [\"Saliency\", \"LRP-Z+\", \"InputXGradient\"],\n",
    "    \"G_GC_LRPeps_GB\": [\"Gradient\", \"LayerGradCam\", \"LRP-Eps\", \"Guided-Backprop\"],\n",
    "    \"GP_GS_GC_LRP-Eps_SA\": [\"Guided-Backprop\", \"GradientShap\", \"LayerGradCam\", \"LRP-Eps\", \"Saliency\"],\n",
    "    \"full_set\": [\"Gradient\", \"Saliency\", \"LayerGradCam\",  \"SmoothGrad\", \"IntegratedGradients\",\n",
    "            \"LRP-Eps\", \"LRP-Z+\", \"Guided-Backprop\", \"GradientShap\", \"InputXGradient\"]\n",
    "}\n",
    "datasets = {\n",
    "    \"MNIST\": {\n",
    "        \"model_name\": \"LeNet\",\n",
    "        \"indices\": [[0,1000]],\n",
    "        },\n",
    "    \"fMNIST\": {\n",
    "        \"model_name\": \"LeNet\",\n",
    "        \"indices\": [[0,1000]],\n",
    "        },\n",
    "     \"ImageNet_ResNet18\": {\n",
    "        \"model_name\": \"ResNet18\",\n",
    "        \"indices\": get_indices(batch_size=100),\n",
    "        },\n",
    "    \"ImageNet_VGG16\": {\n",
    "        \"model_name\": \"VGG16\",\n",
    "        \"indices\": get_indices(batch_size=25),\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac1399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmarking.\n",
    "xai_round = \"G_GC\"\n",
    "fname = xai_round+\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for dataset_name, meta in datasets.items():\n",
    "        dataset_name = dataset_name.split(\"_\")\n",
    "        dataset_name = dataset_name[0]\n",
    "        for i in range(len(meta[\"indices\"])):\n",
    "            torch.cuda.empty_cache()\n",
    "            start_idx, end_idx = meta[\"indices\"][i][0], meta[\"indices\"][i][1]\n",
    "            benchmark = run_benchmarking_script(dataset_name=dataset_name,\n",
    "                                                model_name=meta[\"model_name\"],\n",
    "                                                K=str(5),\n",
    "                                                iters=str(3),\n",
    "                                                xai_methods=xai_methods[xai_round],\n",
    "                                                normalise=True,\n",
    "                                                start_idx=str(start_idx),\n",
    "                                                end_idx=str(end_idx),\n",
    "                                                path_assets=path_assets,\n",
    "                                                path_results=path_results,\n",
    "                                                setup_metrics=setup_estimators,\n",
    "                                                folder=\"benchmarking/\",\n",
    "                                                fname_addition=fname)\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc4052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
